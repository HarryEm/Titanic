---
title: "TitanicFormal"
author: "Harry Emeric"
date: "24/11/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Here I build on my previous Kernel (please see link below) to compare two different approaches to deadling with missing data. Namely, I would like to test whether instead of imputing missing data, training a separate model for each combintaion of observed variables, I improve my predictive accuracy. I made a start on this previously by doing this just for the "Age" variable, and found that it did not improve accuracy, although this could be due to lost information by binning "Age" into brackets. In this work I look to generalise this approach to all variables.

https://www.kaggle.com/harryem/feature-engineering-on-the-titanic-for-0-81339

## Formal delineation of the problem

Given test features $X_{TEST} \in /R^{n * k}$ is a matrix of n test examples and k features and vector $y_{TEST} \in \{0,1\}^n$ classifying survived (1) or did not survive (0), our aim is to estimate a function f such that $y = f(x)$ which minimises misclassification, that is $$ min \sum_{i=1}^{n}(y_i - f(x_i))^2$$ where $f(x_i)$ is the prediction for the ith test example and $y_i$ is the ith classification, which is known only by the Kaggle administrators.